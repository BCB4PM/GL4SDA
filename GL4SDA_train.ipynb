{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+yrEvC+WLD8IS6s00hHLo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BCB4PM/GL4SDA/blob/main/GL4SDA_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and validate the GL4SDA model"
      ],
      "metadata": {
        "id": "abcQG5Dc0Zxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a4nF688L0mbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYQ8dqbK2E8i",
        "outputId": "3b4b1ed5-0090-4a22-9f4c-8e3c84da20cd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_scatter-2.1.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt25cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_sparse-0.6.18%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt25cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_cluster-1.6.3%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Installing collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.3+pt25cu124\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu124.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu124/torch_spline_conv-1.2.2%2Bpt25cu124-cp311-cp311-linux_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.2+pt25cu124\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.12)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import python libraries."
      ],
      "metadata": {
        "id": "6Wtul9jl2pOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv, GATv2Conv, GraphConv, HeteroConv\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "from torch_geometric import seed_everything\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, matthews_corrcoef, f1_score, roc_auc_score"
      ],
      "metadata": {
        "id": "ZdB-w1nN0sTi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get sample data and set seed parameters."
      ],
      "metadata": {
        "id": "JP2K-Nwe3q06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_download_link = \"https://docs.google.com/uc?export=download&id=1SMmwi1WU1WQqTc_PpCyiYGzeVqZrKbeB\"\n",
        "\n",
        "!wget -O hetero_graph_2-4_train.pkl --no-check-certificate \"$file_download_link\"\n",
        "\n",
        "input_graph = \"hetero_graph_2-4_train.pkl\" # training graph input file\n",
        "\n",
        "seed_model = 35 #seed for model training\n",
        "seed = 41 # seed for train/validation splitting"
      ],
      "metadata": {
        "id": "ujzT98qD0hqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b491d7f-35df-40ac-bc70-989f3da26d6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-26 08:34:21--  https://docs.google.com/uc?export=download&id=1SMmwi1WU1WQqTc_PpCyiYGzeVqZrKbeB\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.170.139, 142.251.170.102, 142.251.170.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.170.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1SMmwi1WU1WQqTc_PpCyiYGzeVqZrKbeB&export=download [following]\n",
            "--2025-02-26 08:34:22--  https://drive.usercontent.google.com/download?id=1SMmwi1WU1WQqTc_PpCyiYGzeVqZrKbeB&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 64.233.188.132, 2404:6800:4008:c02::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|64.233.188.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2758761 (2.6M) [application/octet-stream]\n",
            "Saving to: ‘hetero_graph_2-4_train.pkl’\n",
            "\n",
            "hetero_graph_2-4_tr 100%[===================>]   2.63M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-02-26 08:34:25 (56.7 MB/s) - ‘hetero_graph_2-4_train.pkl’ saved [2758761/2758761]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the final classifier.\n",
        "Our final classifier applies the dot-product between source and destination\n",
        "node embeddings to derive edge-level predictions:"
      ],
      "metadata": {
        "id": "P7-C5riZ4nCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "    def forward(self, x_user: Tensor, x_movie: Tensor, edge_label_index: Tensor) -> Tensor:\n",
        "        # Convert node embeddings to edge-level representations:\n",
        "        edge_feat_user = x_user[edge_label_index[0]]\n",
        "        edge_feat_movie = x_movie[edge_label_index[1]]\n",
        "        # Apply dot-product to get a prediction per supervision edge:\n",
        "        return (edge_feat_user * edge_feat_movie).sum(dim=-1)"
      ],
      "metadata": {
        "id": "7p_w0wWZ4yzq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Bipartite GNN model.\n",
        "Class that implements the GNN with GraphConv and GATv2Conv operators, without edge weights"
      ],
      "metadata": {
        "id": "fXmT1PR143Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HeteroGraphGNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, dropout_rate=0.4, num_heads=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "\n",
        "        conv1 = HeteroConv({\n",
        "            ('snorna', 'to', 'disease'): GraphConv((-1, -1), hidden_channels[0]),\n",
        "            ('disease', 'rev_to', 'snorna'): GraphConv((-1, -1), hidden_channels[0]),\n",
        "        }, aggr='sum')\n",
        "        self.convs.append(conv1)\n",
        "\n",
        "        conv2 = HeteroConv({\n",
        "              ('snorna', 'to', 'disease'): GATv2Conv((-1, -1), hidden_channels[1], add_self_loops=False, heads=num_heads, concat=False),\n",
        "              ('disease', 'rev_to', 'snorna'): GATv2Conv((-1, -1), hidden_channels[1], add_self_loops=False, heads=num_heads, concat=False),\n",
        "        }, aggr='sum')\n",
        "        self.convs.append(conv2)\n",
        "\n",
        "\n",
        "        conv3 = HeteroConv({\n",
        "            ('snorna', 'to', 'disease'): GraphConv((-1, -1), hidden_channels[2]),\n",
        "            ('disease', 'rev_to', 'snorna'): GraphConv((-1, -1), hidden_channels[2]),\n",
        "        }, aggr='sum')\n",
        "        self.convs.append(conv3)\n",
        "\n",
        "\n",
        "        self.p = dropout_rate\n",
        "\n",
        "        self.classifier = Classifier()\n",
        "\n",
        "\n",
        "    def forward(self, data, x_dict, edge_index):\n",
        "\n",
        "\n",
        "        for cc, conv in enumerate(self.convs):\n",
        "\n",
        "            x_dict = conv(x_dict, edge_index)\n",
        "\n",
        "            if cc != (len(self.convs)-1):\n",
        "                x_dict = {key: z.relu() for key, z in x_dict.items()}\n",
        "                x_dict = {key: F.dropout(z, p=self.p, training=self.training) for key, z in x_dict.items()}\n",
        "\n",
        "        pred = self.classifier(\n",
        "            x_dict['snorna'],\n",
        "            x_dict['disease'],\n",
        "            data['snorna','to','disease'].edge_label_index\n",
        "        )\n",
        "\n",
        "        return pred"
      ],
      "metadata": {
        "id": "3f4GVNME5Aqq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the model hyperparameters and load the train GNN."
      ],
      "metadata": {
        "id": "CdPayr3d5TEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 500\n",
        "\n",
        "hidden_1 = 128\n",
        "hidden_2 = 128\n",
        "hidden_3 = 64\n",
        "hidden_channels = [hidden_1,hidden_2,hidden_3]\n",
        "\n",
        "lr = 0.0001\n",
        "criterion = torch.nn.BCEWithLogitsLoss() # Choose loss function. We are working directly with logits, i.e. it makes the sigmoid first.\n",
        "\n",
        "# load train graph\n",
        "#!wget \"https://github.com/BCB4PM/GL4SDA/tree/main/data/graph_data/train/hetero_graph_2-4_train.pkl\"\n",
        "#graph = input_graph\n",
        "\n",
        "#graph = torch.load(traindir+input_graph)\n",
        "graph = torch.load(input_graph)\n",
        "\n",
        "\n",
        "#set the seed for reproducibility\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)            # if you are using multi-GPU.\n",
        "np.random.seed(seed)                        # Numpy module.\n",
        "random.seed(seed)                           # Python random module.\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9y8o6M85tOi",
        "outputId": "dd5d4b48-f700-434a-90db-998e1fd3334a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-4dc52ec75507>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  graph = torch.load(input_graph)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split train and validation set and add negative samples to the validation set"
      ],
      "metadata": {
        "id": "IU61aO9A6kJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.RandomLinkSplit(\n",
        "    num_val=0.1,\n",
        "    num_test=0.0,\n",
        "    is_undirected = True,\n",
        "    disjoint_train_ratio=0.2, # ratio between supervision and message passing edges\n",
        "    neg_sampling_ratio=1.0,\n",
        "    add_negative_train_samples=False,\n",
        "    edge_types=(\"snorna\", \"to\", \"disease\"),\n",
        "    rev_edge_types=(\"disease\", \"rev_to\", \"snorna\"),\n",
        "    )\n",
        "\n",
        "train_data, val_data, empty_data = transform(graph)\n",
        "\n",
        "\n",
        "# Define train seed edges and add negative samples to the training set:\n",
        "edge_label_index = train_data[\"snorna\", \"to\", \"disease\"].edge_label_index\n",
        "edge_label = train_data[\"snorna\", \"to\", \"disease\"].edge_label\n",
        "train_loader = LinkNeighborLoader(\n",
        "    data=train_data,\n",
        "    num_neighbors=[20, 15],\n",
        "    neg_sampling_ratio=2.0,\n",
        "    edge_label_index=((\"snorna\", \"to\", \"disease\"), edge_label_index),\n",
        "    edge_label=edge_label,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# set the seed for the GNN model\n",
        "seed_everything(seed_model)"
      ],
      "metadata": {
        "id": "OG-mqSKH6mii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325c9362-ab59-4895-ff62-287e0aa87842"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
            "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate the model and start training."
      ],
      "metadata": {
        "id": "QSCg9BAY6zTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = HeteroGraphGNN(hidden_channels=hidden_channels)\n",
        "model = model.double()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr) # Choose optimizer.\n",
        "\n",
        "# start training\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = total_examples = 0\n",
        "    for sampled_data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        sampled_data.to(device)\n",
        "        pred = model(sampled_data,sampled_data.x_dict,sampled_data.edge_index_dict)\n",
        "        ground_truth = sampled_data[\"snorna\", \"to\", \"disease\"].edge_label\n",
        "        loss = criterion(pred, ground_truth)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss) * pred.numel()\n",
        "        total_examples += pred.numel()\n",
        "    print(f\"Epoch: {epoch:03d}, Loss: {total_loss / total_examples:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaOY_UUC66dc",
        "outputId": "872a915a-718f-436d-9720-b4e2bf58a08e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 0.6899\n",
            "Epoch: 001, Loss: 0.6819\n",
            "Epoch: 002, Loss: 0.7001\n",
            "Epoch: 003, Loss: 0.6940\n",
            "Epoch: 004, Loss: 0.6691\n",
            "Epoch: 005, Loss: 0.6781\n",
            "Epoch: 006, Loss: 0.6753\n",
            "Epoch: 007, Loss: 0.6657\n",
            "Epoch: 008, Loss: 0.6795\n",
            "Epoch: 009, Loss: 0.6754\n",
            "Epoch: 010, Loss: 0.6685\n",
            "Epoch: 011, Loss: 0.6568\n",
            "Epoch: 012, Loss: 0.6581\n",
            "Epoch: 013, Loss: 0.6626\n",
            "Epoch: 014, Loss: 0.6534\n",
            "Epoch: 015, Loss: 0.6469\n",
            "Epoch: 016, Loss: 0.6406\n",
            "Epoch: 017, Loss: 0.6701\n",
            "Epoch: 018, Loss: 0.6474\n",
            "Epoch: 019, Loss: 0.6344\n",
            "Epoch: 020, Loss: 0.6169\n",
            "Epoch: 021, Loss: 0.6194\n",
            "Epoch: 022, Loss: 0.5958\n",
            "Epoch: 023, Loss: 0.6110\n",
            "Epoch: 024, Loss: 0.6108\n",
            "Epoch: 025, Loss: 0.5977\n",
            "Epoch: 026, Loss: 0.5760\n",
            "Epoch: 027, Loss: 0.5477\n",
            "Epoch: 028, Loss: 0.5479\n",
            "Epoch: 029, Loss: 0.5939\n",
            "Epoch: 030, Loss: 0.5921\n",
            "Epoch: 031, Loss: 0.5537\n",
            "Epoch: 032, Loss: 0.5641\n",
            "Epoch: 033, Loss: 0.5454\n",
            "Epoch: 034, Loss: 0.5830\n",
            "Epoch: 035, Loss: 0.5773\n",
            "Epoch: 036, Loss: 0.5524\n",
            "Epoch: 037, Loss: 0.5507\n",
            "Epoch: 038, Loss: 0.5492\n",
            "Epoch: 039, Loss: 0.5769\n",
            "Epoch: 040, Loss: 0.6273\n",
            "Epoch: 041, Loss: 0.5518\n",
            "Epoch: 042, Loss: 0.5999\n",
            "Epoch: 043, Loss: 0.6213\n",
            "Epoch: 044, Loss: 0.5735\n",
            "Epoch: 045, Loss: 0.5573\n",
            "Epoch: 046, Loss: 0.5924\n",
            "Epoch: 047, Loss: 0.5403\n",
            "Epoch: 048, Loss: 0.5013\n",
            "Epoch: 049, Loss: 0.5545\n",
            "Epoch: 050, Loss: 0.5345\n",
            "Epoch: 051, Loss: 0.5407\n",
            "Epoch: 052, Loss: 0.5354\n",
            "Epoch: 053, Loss: 0.5347\n",
            "Epoch: 054, Loss: 0.5208\n",
            "Epoch: 055, Loss: 0.5570\n",
            "Epoch: 056, Loss: 0.5247\n",
            "Epoch: 057, Loss: 0.5628\n",
            "Epoch: 058, Loss: 0.5409\n",
            "Epoch: 059, Loss: 0.5236\n",
            "Epoch: 060, Loss: 0.5509\n",
            "Epoch: 061, Loss: 0.5352\n",
            "Epoch: 062, Loss: 0.5345\n",
            "Epoch: 063, Loss: 0.5147\n",
            "Epoch: 064, Loss: 0.5467\n",
            "Epoch: 065, Loss: 0.5598\n",
            "Epoch: 066, Loss: 0.5432\n",
            "Epoch: 067, Loss: 0.5478\n",
            "Epoch: 068, Loss: 0.5228\n",
            "Epoch: 069, Loss: 0.5141\n",
            "Epoch: 070, Loss: 0.5233\n",
            "Epoch: 071, Loss: 0.5112\n",
            "Epoch: 072, Loss: 0.5134\n",
            "Epoch: 073, Loss: 0.5095\n",
            "Epoch: 074, Loss: 0.5266\n",
            "Epoch: 075, Loss: 0.5411\n",
            "Epoch: 076, Loss: 0.5095\n",
            "Epoch: 077, Loss: 0.5357\n",
            "Epoch: 078, Loss: 0.4962\n",
            "Epoch: 079, Loss: 0.5019\n",
            "Epoch: 080, Loss: 0.5215\n",
            "Epoch: 081, Loss: 0.4893\n",
            "Epoch: 082, Loss: 0.5898\n",
            "Epoch: 083, Loss: 0.5143\n",
            "Epoch: 084, Loss: 0.5100\n",
            "Epoch: 085, Loss: 0.5312\n",
            "Epoch: 086, Loss: 0.5054\n",
            "Epoch: 087, Loss: 0.5120\n",
            "Epoch: 088, Loss: 0.5100\n",
            "Epoch: 089, Loss: 0.5245\n",
            "Epoch: 090, Loss: 0.5266\n",
            "Epoch: 091, Loss: 0.4922\n",
            "Epoch: 092, Loss: 0.4853\n",
            "Epoch: 093, Loss: 0.4943\n",
            "Epoch: 094, Loss: 0.4716\n",
            "Epoch: 095, Loss: 0.4867\n",
            "Epoch: 096, Loss: 0.5229\n",
            "Epoch: 097, Loss: 0.5098\n",
            "Epoch: 098, Loss: 0.5023\n",
            "Epoch: 099, Loss: 0.5193\n",
            "Epoch: 100, Loss: 0.5219\n",
            "Epoch: 101, Loss: 0.4893\n",
            "Epoch: 102, Loss: 0.5061\n",
            "Epoch: 103, Loss: 0.5260\n",
            "Epoch: 104, Loss: 0.4841\n",
            "Epoch: 105, Loss: 0.5256\n",
            "Epoch: 106, Loss: 0.5145\n",
            "Epoch: 107, Loss: 0.4999\n",
            "Epoch: 108, Loss: 0.4844\n",
            "Epoch: 109, Loss: 0.4929\n",
            "Epoch: 110, Loss: 0.4762\n",
            "Epoch: 111, Loss: 0.4726\n",
            "Epoch: 112, Loss: 0.5380\n",
            "Epoch: 113, Loss: 0.5245\n",
            "Epoch: 114, Loss: 0.4596\n",
            "Epoch: 115, Loss: 0.5698\n",
            "Epoch: 116, Loss: 0.5134\n",
            "Epoch: 117, Loss: 0.4494\n",
            "Epoch: 118, Loss: 0.4931\n",
            "Epoch: 119, Loss: 0.5060\n",
            "Epoch: 120, Loss: 0.5559\n",
            "Epoch: 121, Loss: 0.4745\n",
            "Epoch: 122, Loss: 0.4823\n",
            "Epoch: 123, Loss: 0.5059\n",
            "Epoch: 124, Loss: 0.4956\n",
            "Epoch: 125, Loss: 0.4926\n",
            "Epoch: 126, Loss: 0.4344\n",
            "Epoch: 127, Loss: 0.4829\n",
            "Epoch: 128, Loss: 0.4610\n",
            "Epoch: 129, Loss: 0.4644\n",
            "Epoch: 130, Loss: 0.4913\n",
            "Epoch: 131, Loss: 0.4693\n",
            "Epoch: 132, Loss: 0.4461\n",
            "Epoch: 133, Loss: 0.5065\n",
            "Epoch: 134, Loss: 0.4661\n",
            "Epoch: 135, Loss: 0.4885\n",
            "Epoch: 136, Loss: 0.4588\n",
            "Epoch: 137, Loss: 0.4475\n",
            "Epoch: 138, Loss: 0.4399\n",
            "Epoch: 139, Loss: 0.4737\n",
            "Epoch: 140, Loss: 0.4720\n",
            "Epoch: 141, Loss: 0.4589\n",
            "Epoch: 142, Loss: 0.4575\n",
            "Epoch: 143, Loss: 0.4891\n",
            "Epoch: 144, Loss: 0.4689\n",
            "Epoch: 145, Loss: 0.4747\n",
            "Epoch: 146, Loss: 0.4472\n",
            "Epoch: 147, Loss: 0.4594\n",
            "Epoch: 148, Loss: 0.4351\n",
            "Epoch: 149, Loss: 0.4915\n",
            "Epoch: 150, Loss: 0.4912\n",
            "Epoch: 151, Loss: 0.4838\n",
            "Epoch: 152, Loss: 0.4363\n",
            "Epoch: 153, Loss: 0.4364\n",
            "Epoch: 154, Loss: 0.4575\n",
            "Epoch: 155, Loss: 0.4556\n",
            "Epoch: 156, Loss: 0.4697\n",
            "Epoch: 157, Loss: 0.4979\n",
            "Epoch: 158, Loss: 0.4556\n",
            "Epoch: 159, Loss: 0.4556\n",
            "Epoch: 160, Loss: 0.4765\n",
            "Epoch: 161, Loss: 0.4546\n",
            "Epoch: 162, Loss: 0.5050\n",
            "Epoch: 163, Loss: 0.4336\n",
            "Epoch: 164, Loss: 0.4533\n",
            "Epoch: 165, Loss: 0.4845\n",
            "Epoch: 166, Loss: 0.4931\n",
            "Epoch: 167, Loss: 0.4471\n",
            "Epoch: 168, Loss: 0.4668\n",
            "Epoch: 169, Loss: 0.4581\n",
            "Epoch: 170, Loss: 0.4797\n",
            "Epoch: 171, Loss: 0.4543\n",
            "Epoch: 172, Loss: 0.5035\n",
            "Epoch: 173, Loss: 0.4505\n",
            "Epoch: 174, Loss: 0.4722\n",
            "Epoch: 175, Loss: 0.4663\n",
            "Epoch: 176, Loss: 0.4377\n",
            "Epoch: 177, Loss: 0.4236\n",
            "Epoch: 178, Loss: 0.4733\n",
            "Epoch: 179, Loss: 0.4543\n",
            "Epoch: 180, Loss: 0.4458\n",
            "Epoch: 181, Loss: 0.5034\n",
            "Epoch: 182, Loss: 0.4410\n",
            "Epoch: 183, Loss: 0.4418\n",
            "Epoch: 184, Loss: 0.4499\n",
            "Epoch: 185, Loss: 0.4816\n",
            "Epoch: 186, Loss: 0.4785\n",
            "Epoch: 187, Loss: 0.4259\n",
            "Epoch: 188, Loss: 0.4764\n",
            "Epoch: 189, Loss: 0.4663\n",
            "Epoch: 190, Loss: 0.4284\n",
            "Epoch: 191, Loss: 0.4833\n",
            "Epoch: 192, Loss: 0.4131\n",
            "Epoch: 193, Loss: 0.4534\n",
            "Epoch: 194, Loss: 0.4406\n",
            "Epoch: 195, Loss: 0.4406\n",
            "Epoch: 196, Loss: 0.4748\n",
            "Epoch: 197, Loss: 0.4569\n",
            "Epoch: 198, Loss: 0.4527\n",
            "Epoch: 199, Loss: 0.4600\n",
            "Epoch: 200, Loss: 0.4765\n",
            "Epoch: 201, Loss: 0.4486\n",
            "Epoch: 202, Loss: 0.4520\n",
            "Epoch: 203, Loss: 0.4367\n",
            "Epoch: 204, Loss: 0.4540\n",
            "Epoch: 205, Loss: 0.4927\n",
            "Epoch: 206, Loss: 0.4351\n",
            "Epoch: 207, Loss: 0.4560\n",
            "Epoch: 208, Loss: 0.4838\n",
            "Epoch: 209, Loss: 0.4228\n",
            "Epoch: 210, Loss: 0.4695\n",
            "Epoch: 211, Loss: 0.4369\n",
            "Epoch: 212, Loss: 0.4372\n",
            "Epoch: 213, Loss: 0.4304\n",
            "Epoch: 214, Loss: 0.4309\n",
            "Epoch: 215, Loss: 0.4434\n",
            "Epoch: 216, Loss: 0.3798\n",
            "Epoch: 217, Loss: 0.4388\n",
            "Epoch: 218, Loss: 0.4266\n",
            "Epoch: 219, Loss: 0.4302\n",
            "Epoch: 220, Loss: 0.4456\n",
            "Epoch: 221, Loss: 0.4691\n",
            "Epoch: 222, Loss: 0.4955\n",
            "Epoch: 223, Loss: 0.4692\n",
            "Epoch: 224, Loss: 0.4543\n",
            "Epoch: 225, Loss: 0.4185\n",
            "Epoch: 226, Loss: 0.4562\n",
            "Epoch: 227, Loss: 0.4760\n",
            "Epoch: 228, Loss: 0.4426\n",
            "Epoch: 229, Loss: 0.4087\n",
            "Epoch: 230, Loss: 0.4249\n",
            "Epoch: 231, Loss: 0.4035\n",
            "Epoch: 232, Loss: 0.4476\n",
            "Epoch: 233, Loss: 0.4265\n",
            "Epoch: 234, Loss: 0.4033\n",
            "Epoch: 235, Loss: 0.4125\n",
            "Epoch: 236, Loss: 0.4529\n",
            "Epoch: 237, Loss: 0.4416\n",
            "Epoch: 238, Loss: 0.4325\n",
            "Epoch: 239, Loss: 0.4415\n",
            "Epoch: 240, Loss: 0.4212\n",
            "Epoch: 241, Loss: 0.4233\n",
            "Epoch: 242, Loss: 0.4359\n",
            "Epoch: 243, Loss: 0.4485\n",
            "Epoch: 244, Loss: 0.4232\n",
            "Epoch: 245, Loss: 0.4339\n",
            "Epoch: 246, Loss: 0.4482\n",
            "Epoch: 247, Loss: 0.4166\n",
            "Epoch: 248, Loss: 0.4691\n",
            "Epoch: 249, Loss: 0.4675\n",
            "Epoch: 250, Loss: 0.4309\n",
            "Epoch: 251, Loss: 0.4238\n",
            "Epoch: 252, Loss: 0.4526\n",
            "Epoch: 253, Loss: 0.4425\n",
            "Epoch: 254, Loss: 0.4165\n",
            "Epoch: 255, Loss: 0.4186\n",
            "Epoch: 256, Loss: 0.4155\n",
            "Epoch: 257, Loss: 0.3857\n",
            "Epoch: 258, Loss: 0.4352\n",
            "Epoch: 259, Loss: 0.4195\n",
            "Epoch: 260, Loss: 0.4142\n",
            "Epoch: 261, Loss: 0.4268\n",
            "Epoch: 262, Loss: 0.4115\n",
            "Epoch: 263, Loss: 0.4423\n",
            "Epoch: 264, Loss: 0.4433\n",
            "Epoch: 265, Loss: 0.3971\n",
            "Epoch: 266, Loss: 0.4058\n",
            "Epoch: 267, Loss: 0.3742\n",
            "Epoch: 268, Loss: 0.4282\n",
            "Epoch: 269, Loss: 0.4050\n",
            "Epoch: 270, Loss: 0.4316\n",
            "Epoch: 271, Loss: 0.4386\n",
            "Epoch: 272, Loss: 0.4241\n",
            "Epoch: 273, Loss: 0.4110\n",
            "Epoch: 274, Loss: 0.4348\n",
            "Epoch: 275, Loss: 0.4049\n",
            "Epoch: 276, Loss: 0.4089\n",
            "Epoch: 277, Loss: 0.4328\n",
            "Epoch: 278, Loss: 0.4556\n",
            "Epoch: 279, Loss: 0.4026\n",
            "Epoch: 280, Loss: 0.4188\n",
            "Epoch: 281, Loss: 0.4057\n",
            "Epoch: 282, Loss: 0.3683\n",
            "Epoch: 283, Loss: 0.4270\n",
            "Epoch: 284, Loss: 0.4709\n",
            "Epoch: 285, Loss: 0.3944\n",
            "Epoch: 286, Loss: 0.3946\n",
            "Epoch: 287, Loss: 0.4050\n",
            "Epoch: 288, Loss: 0.4475\n",
            "Epoch: 289, Loss: 0.4073\n",
            "Epoch: 290, Loss: 0.4422\n",
            "Epoch: 291, Loss: 0.4228\n",
            "Epoch: 292, Loss: 0.4753\n",
            "Epoch: 293, Loss: 0.4069\n",
            "Epoch: 294, Loss: 0.4348\n",
            "Epoch: 295, Loss: 0.4156\n",
            "Epoch: 296, Loss: 0.4458\n",
            "Epoch: 297, Loss: 0.4106\n",
            "Epoch: 298, Loss: 0.4023\n",
            "Epoch: 299, Loss: 0.4129\n",
            "Epoch: 300, Loss: 0.4122\n",
            "Epoch: 301, Loss: 0.4152\n",
            "Epoch: 302, Loss: 0.4031\n",
            "Epoch: 303, Loss: 0.3866\n",
            "Epoch: 304, Loss: 0.4146\n",
            "Epoch: 305, Loss: 0.3951\n",
            "Epoch: 306, Loss: 0.4334\n",
            "Epoch: 307, Loss: 0.3656\n",
            "Epoch: 308, Loss: 0.3988\n",
            "Epoch: 309, Loss: 0.4306\n",
            "Epoch: 310, Loss: 0.4253\n",
            "Epoch: 311, Loss: 0.3973\n",
            "Epoch: 312, Loss: 0.4123\n",
            "Epoch: 313, Loss: 0.4117\n",
            "Epoch: 314, Loss: 0.4183\n",
            "Epoch: 315, Loss: 0.4303\n",
            "Epoch: 316, Loss: 0.3963\n",
            "Epoch: 317, Loss: 0.3687\n",
            "Epoch: 318, Loss: 0.3854\n",
            "Epoch: 319, Loss: 0.4373\n",
            "Epoch: 320, Loss: 0.4097\n",
            "Epoch: 321, Loss: 0.3796\n",
            "Epoch: 322, Loss: 0.3994\n",
            "Epoch: 323, Loss: 0.3844\n",
            "Epoch: 324, Loss: 0.4197\n",
            "Epoch: 325, Loss: 0.4070\n",
            "Epoch: 326, Loss: 0.4006\n",
            "Epoch: 327, Loss: 0.3908\n",
            "Epoch: 328, Loss: 0.4070\n",
            "Epoch: 329, Loss: 0.3673\n",
            "Epoch: 330, Loss: 0.4550\n",
            "Epoch: 331, Loss: 0.4220\n",
            "Epoch: 332, Loss: 0.3980\n",
            "Epoch: 333, Loss: 0.3903\n",
            "Epoch: 334, Loss: 0.4288\n",
            "Epoch: 335, Loss: 0.4079\n",
            "Epoch: 336, Loss: 0.4229\n",
            "Epoch: 337, Loss: 0.4390\n",
            "Epoch: 338, Loss: 0.4031\n",
            "Epoch: 339, Loss: 0.3886\n",
            "Epoch: 340, Loss: 0.3879\n",
            "Epoch: 341, Loss: 0.4101\n",
            "Epoch: 342, Loss: 0.3766\n",
            "Epoch: 343, Loss: 0.4013\n",
            "Epoch: 344, Loss: 0.4062\n",
            "Epoch: 345, Loss: 0.4170\n",
            "Epoch: 346, Loss: 0.4683\n",
            "Epoch: 347, Loss: 0.4048\n",
            "Epoch: 348, Loss: 0.3939\n",
            "Epoch: 349, Loss: 0.4255\n",
            "Epoch: 350, Loss: 0.4312\n",
            "Epoch: 351, Loss: 0.4199\n",
            "Epoch: 352, Loss: 0.4492\n",
            "Epoch: 353, Loss: 0.4590\n",
            "Epoch: 354, Loss: 0.3919\n",
            "Epoch: 355, Loss: 0.4139\n",
            "Epoch: 356, Loss: 0.4106\n",
            "Epoch: 357, Loss: 0.4281\n",
            "Epoch: 358, Loss: 0.4282\n",
            "Epoch: 359, Loss: 0.4219\n",
            "Epoch: 360, Loss: 0.4044\n",
            "Epoch: 361, Loss: 0.3721\n",
            "Epoch: 362, Loss: 0.3889\n",
            "Epoch: 363, Loss: 0.3881\n",
            "Epoch: 364, Loss: 0.4015\n",
            "Epoch: 365, Loss: 0.4029\n",
            "Epoch: 366, Loss: 0.4209\n",
            "Epoch: 367, Loss: 0.3937\n",
            "Epoch: 368, Loss: 0.3719\n",
            "Epoch: 369, Loss: 0.3533\n",
            "Epoch: 370, Loss: 0.4135\n",
            "Epoch: 371, Loss: 0.3697\n",
            "Epoch: 372, Loss: 0.3894\n",
            "Epoch: 373, Loss: 0.3926\n",
            "Epoch: 374, Loss: 0.4171\n",
            "Epoch: 375, Loss: 0.3776\n",
            "Epoch: 376, Loss: 0.3981\n",
            "Epoch: 377, Loss: 0.3829\n",
            "Epoch: 378, Loss: 0.3996\n",
            "Epoch: 379, Loss: 0.3887\n",
            "Epoch: 380, Loss: 0.4172\n",
            "Epoch: 381, Loss: 0.4549\n",
            "Epoch: 382, Loss: 0.3981\n",
            "Epoch: 383, Loss: 0.4280\n",
            "Epoch: 384, Loss: 0.4229\n",
            "Epoch: 385, Loss: 0.3942\n",
            "Epoch: 386, Loss: 0.4136\n",
            "Epoch: 387, Loss: 0.4080\n",
            "Epoch: 388, Loss: 0.4079\n",
            "Epoch: 389, Loss: 0.4033\n",
            "Epoch: 390, Loss: 0.4110\n",
            "Epoch: 391, Loss: 0.4424\n",
            "Epoch: 392, Loss: 0.3923\n",
            "Epoch: 393, Loss: 0.4172\n",
            "Epoch: 394, Loss: 0.3652\n",
            "Epoch: 395, Loss: 0.3871\n",
            "Epoch: 396, Loss: 0.3567\n",
            "Epoch: 397, Loss: 0.4326\n",
            "Epoch: 398, Loss: 0.3823\n",
            "Epoch: 399, Loss: 0.3624\n",
            "Epoch: 400, Loss: 0.4078\n",
            "Epoch: 401, Loss: 0.3946\n",
            "Epoch: 402, Loss: 0.4094\n",
            "Epoch: 403, Loss: 0.4056\n",
            "Epoch: 404, Loss: 0.3780\n",
            "Epoch: 405, Loss: 0.4017\n",
            "Epoch: 406, Loss: 0.3627\n",
            "Epoch: 407, Loss: 0.3984\n",
            "Epoch: 408, Loss: 0.3731\n",
            "Epoch: 409, Loss: 0.3914\n",
            "Epoch: 410, Loss: 0.3742\n",
            "Epoch: 411, Loss: 0.3855\n",
            "Epoch: 412, Loss: 0.4019\n",
            "Epoch: 413, Loss: 0.3687\n",
            "Epoch: 414, Loss: 0.4090\n",
            "Epoch: 415, Loss: 0.3616\n",
            "Epoch: 416, Loss: 0.4068\n",
            "Epoch: 417, Loss: 0.4017\n",
            "Epoch: 418, Loss: 0.4101\n",
            "Epoch: 419, Loss: 0.3928\n",
            "Epoch: 420, Loss: 0.3742\n",
            "Epoch: 421, Loss: 0.3991\n",
            "Epoch: 422, Loss: 0.3809\n",
            "Epoch: 423, Loss: 0.3982\n",
            "Epoch: 424, Loss: 0.4015\n",
            "Epoch: 425, Loss: 0.4054\n",
            "Epoch: 426, Loss: 0.4113\n",
            "Epoch: 427, Loss: 0.3692\n",
            "Epoch: 428, Loss: 0.4010\n",
            "Epoch: 429, Loss: 0.4001\n",
            "Epoch: 430, Loss: 0.3890\n",
            "Epoch: 431, Loss: 0.4081\n",
            "Epoch: 432, Loss: 0.3821\n",
            "Epoch: 433, Loss: 0.3934\n",
            "Epoch: 434, Loss: 0.4133\n",
            "Epoch: 435, Loss: 0.3758\n",
            "Epoch: 436, Loss: 0.4216\n",
            "Epoch: 437, Loss: 0.3521\n",
            "Epoch: 438, Loss: 0.3724\n",
            "Epoch: 439, Loss: 0.3666\n",
            "Epoch: 440, Loss: 0.3569\n",
            "Epoch: 441, Loss: 0.3972\n",
            "Epoch: 442, Loss: 0.3829\n",
            "Epoch: 443, Loss: 0.3816\n",
            "Epoch: 444, Loss: 0.3525\n",
            "Epoch: 445, Loss: 0.3761\n",
            "Epoch: 446, Loss: 0.3660\n",
            "Epoch: 447, Loss: 0.3631\n",
            "Epoch: 448, Loss: 0.3597\n",
            "Epoch: 449, Loss: 0.4008\n",
            "Epoch: 450, Loss: 0.3697\n",
            "Epoch: 451, Loss: 0.3972\n",
            "Epoch: 452, Loss: 0.3966\n",
            "Epoch: 453, Loss: 0.3963\n",
            "Epoch: 454, Loss: 0.3672\n",
            "Epoch: 455, Loss: 0.3867\n",
            "Epoch: 456, Loss: 0.3723\n",
            "Epoch: 457, Loss: 0.3862\n",
            "Epoch: 458, Loss: 0.4016\n",
            "Epoch: 459, Loss: 0.3887\n",
            "Epoch: 460, Loss: 0.3552\n",
            "Epoch: 461, Loss: 0.3689\n",
            "Epoch: 462, Loss: 0.3934\n",
            "Epoch: 463, Loss: 0.4183\n",
            "Epoch: 464, Loss: 0.3861\n",
            "Epoch: 465, Loss: 0.3877\n",
            "Epoch: 466, Loss: 0.3669\n",
            "Epoch: 467, Loss: 0.3858\n",
            "Epoch: 468, Loss: 0.3449\n",
            "Epoch: 469, Loss: 0.3509\n",
            "Epoch: 470, Loss: 0.4075\n",
            "Epoch: 471, Loss: 0.4017\n",
            "Epoch: 472, Loss: 0.3974\n",
            "Epoch: 473, Loss: 0.3769\n",
            "Epoch: 474, Loss: 0.3645\n",
            "Epoch: 475, Loss: 0.3729\n",
            "Epoch: 476, Loss: 0.3988\n",
            "Epoch: 477, Loss: 0.4027\n",
            "Epoch: 478, Loss: 0.3920\n",
            "Epoch: 479, Loss: 0.3626\n",
            "Epoch: 480, Loss: 0.3941\n",
            "Epoch: 481, Loss: 0.4192\n",
            "Epoch: 482, Loss: 0.3538\n",
            "Epoch: 483, Loss: 0.4152\n",
            "Epoch: 484, Loss: 0.4039\n",
            "Epoch: 485, Loss: 0.4158\n",
            "Epoch: 486, Loss: 0.3662\n",
            "Epoch: 487, Loss: 0.3930\n",
            "Epoch: 488, Loss: 0.3744\n",
            "Epoch: 489, Loss: 0.3910\n",
            "Epoch: 490, Loss: 0.3748\n",
            "Epoch: 491, Loss: 0.3730\n",
            "Epoch: 492, Loss: 0.3715\n",
            "Epoch: 493, Loss: 0.3856\n",
            "Epoch: 494, Loss: 0.3663\n",
            "Epoch: 495, Loss: 0.3880\n",
            "Epoch: 496, Loss: 0.3667\n",
            "Epoch: 497, Loss: 0.3765\n",
            "Epoch: 498, Loss: 0.3711\n",
            "Epoch: 499, Loss: 0.3672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate GL4SDA model.\n",
        "We do not need to add negative samples for the validation set. It already has them."
      ],
      "metadata": {
        "id": "hw4IN0om7R3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define validation seed edges:\n",
        "edge_label_index = val_data[\"snorna\", \"to\", \"disease\"].edge_label_index\n",
        "edge_label = val_data[\"snorna\", \"to\", \"disease\"].edge_label\n",
        "val_loader = LinkNeighborLoader(\n",
        "    data=val_data,\n",
        "    num_neighbors=[20, 15],\n",
        "    edge_label_index=((\"snorna\", \"to\", \"disease\"), edge_label_index),\n",
        "    edge_label=edge_label,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "preds = []\n",
        "ground_truths = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for sampled_val_data in val_loader:\n",
        "        sampled_val_data.to(device)\n",
        "        preds.append(model(sampled_val_data,sampled_val_data.x_dict,sampled_val_data.edge_index_dict))\n",
        "        ground_truths.append(sampled_val_data[\"snorna\", \"to\", \"disease\"].edge_label)\n",
        "pred_row = torch.cat(preds, dim=0).cpu()\n",
        "pred = torch.cat(preds, dim=0).cpu().numpy()\n",
        "ground_row = torch.cat(ground_truths, dim=0).cpu()\n",
        "ground_truth = torch.cat(ground_truths, dim=0).cpu().numpy()\n",
        "auc_val = roc_auc_score(ground_truth, pred)\n",
        "acc_val = accuracy_score( ground_truth, pred_row.sigmoid().round() )\n",
        "val_pre = precision_score(ground_truth,pred_row.sigmoid().round())\n",
        "val_rec = recall_score(ground_truth,pred_row.sigmoid().round())\n",
        "val_mcc = matthews_corrcoef(ground_truth,pred_row.sigmoid().round())\n",
        "val_f1 = f1_score(ground_truth,pred_row.sigmoid().round())"
      ],
      "metadata": {
        "id": "XSRzUSti7f86"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print validated output\n",
        "print()\n",
        "print(f\"Validation AUC: {auc_val:.4f}\")\n",
        "print(f\"Validation ACC: {acc_val:.4f}\")\n",
        "print(f\"Validation Prec: {val_pre:.4f}\")\n",
        "print(f\"Validation Recall: {val_rec:.4f}\")\n",
        "print(f\"Validation MCC: {val_mcc:.4f}\")\n",
        "print(f\"Validation F-1: {val_f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aa0Szsh7oTa",
        "outputId": "75f85f16-dc26-49ea-e0ce-5c79f7b9383d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation AUC: 0.9126\n",
            "Validation ACC: 0.8354\n",
            "Validation Prec: 0.9365\n",
            "Validation Recall: 0.7195\n",
            "Validation MCC: 0.6895\n",
            "Validation F-1: 0.8138\n"
          ]
        }
      ]
    }
  ]
}